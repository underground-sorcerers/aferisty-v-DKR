{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y8utaGr-z9-s",
        "outputId": "f3882f7a-dece-489e-a1bb-53bd171e6940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-l19zpw5a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-l19zpw5a\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 46705844b39ededc0fcef1de90e73923480a6446\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.7.0) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.7.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.7.0) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric==2.7.0) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "umZifMrxz_mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tabular dataset\n",
        "df = pd.read_csv(\"credit_card_transactions-ibm_v2.csv\").sample(n=100000, random_state=42)\n",
        "df[\"card_id\"] = df[\"User\"].astype(str) + \"_\" + df[\"Card\"].astype(str)\n",
        "df[\"Amount\"]=df[\"Amount\"].str.replace(\"$\",\"\").astype(float)\n",
        "df[\"Hour\"] = df[\"Time\"].str [0:2]\n",
        "df[\"Minute\"] = df[\"Time\"].str [3:5]\n",
        "df = df.drop([\"Time\",\"User\",\"Card\"],axis=1)\n",
        "df[\"Errors?\"]= df[\"Errors?\"].fillna(\"No error\")\n",
        "df = df.drop(columns=[\"Merchant State\",\"Zip\"],axis=1)\n",
        "df[\"Is Fraud?\"] = df[\"Is Fraud?\"].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "df[\"Merchant City\"]=LabelEncoder().fit_transform(df[\"Merchant City\"])\n",
        "df[\"Use Chip\"]=LabelEncoder().fit_transform(df[\"Use Chip\"])\n",
        "df[\"Errors?\"]=LabelEncoder().fit_transform(df[\"Errors?\"])"
      ],
      "metadata": {
        "id": "5BWHY6m50F0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fraudulent = df[df[\"Is Fraud?\"] == 1]\n",
        "# non_fraudulent = df[df[\"Is Fraud?\"] == 0]\n",
        "\n",
        "# print(f\"Number of fraudulent transactions: {len(fraudulent)}\")\n",
        "# print(f\"Number of non-fraudulent transactions: {len(non_fraudulent)}\")\n",
        "\n",
        "# desired_ratio = 1  # Ratio of non-fraud to fraud (e.g., 1 means 1:1 balance)\n",
        "# non_fraud_sample_size = len(fraudulent) * desired_ratio\n",
        "\n",
        "# non_fraudulent_sampled = non_fraudulent.sample(n=non_fraud_sample_size, random_state=42)\n",
        "\n",
        "# balanced_df = pd.concat([fraudulent, non_fraudulent_sampled])\n",
        "# balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
        "\n",
        "# print(f\"Balanced dataset size: {len(balanced_df)}\")\n",
        "# print(balanced_df[\"Is Fraud?\"].value_counts())\n",
        "# df = balanced_df.copy()"
      ],
      "metadata": {
        "id": "uoTD3e71jF5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConstruction:\n",
        "    def __init__(self, nodes, edges, features=None):\n",
        "        self.g_nx = nx.DiGraph()\n",
        "        self.add_nodes(nodes)\n",
        "        self.add_edges(edges)\n",
        "        self.node_features = features if features is not None else {}\n",
        "\n",
        "    def add_nodes(self, nodes):\n",
        "        for ntype, nodelist in nodes.items():\n",
        "            for node in nodelist:\n",
        "                self.g_nx.add_node(node, ntype=ntype)\n",
        "\n",
        "    def add_edges(self, edges):\n",
        "        for edge in edges:\n",
        "            self.g_nx.add_edges_from(edge)\n",
        "\n",
        "    def to_pyg_data(self):\n",
        "        data = HeteroData()\n",
        "        node_id_mapping = {ntype: [] for ntype in set(nx.get_node_attributes(self.g_nx, 'ntype').values())}\n",
        "\n",
        "        print(\"Processing nodes...\")\n",
        "        for node, ndata in tqdm(self.g_nx.nodes(data=True), desc=\"Nodes\"):\n",
        "            ntype = ndata['ntype']\n",
        "            if 'x' not in data[ntype]:\n",
        "                data[ntype].x = []\n",
        "            node_id_mapping[ntype].append(node)\n",
        "            if self.node_features and ntype in self.node_features:\n",
        "                if node in self.node_features[ntype].index:\n",
        "                    features = self.node_features[ntype].loc[node].values\n",
        "                    features = [float(f) if isinstance(f, (int, float)) else 0.0 for f in features]\n",
        "                    data[ntype].x.append(features)\n",
        "                else:\n",
        "                    data[ntype].x.append([0.0] * self.node_features[ntype].shape[1])\n",
        "\n",
        "        print(\"Converting node features to tensors...\")\n",
        "        for ntype in tqdm(data.node_types, desc=\"Node Types\"):\n",
        "            data[ntype].x = torch.tensor(data[ntype].x, dtype=torch.float)\n",
        "\n",
        "        print(\"Processing edges...\")\n",
        "        for u, v in tqdm(self.g_nx.edges(), desc=\"Edges\"):\n",
        "            u_type = self.g_nx.nodes[u]['ntype']\n",
        "            v_type = self.g_nx.nodes[v]['ntype']\n",
        "            edge_type = (u_type, 'to', v_type)\n",
        "            if edge_type not in data.edge_types:\n",
        "                data[edge_type].edge_index = [[], []]\n",
        "            if u in node_id_mapping[u_type] and v in node_id_mapping[v_type]:\n",
        "                u_index = node_id_mapping[u_type].index(u)\n",
        "                v_index = node_id_mapping[v_type].index(v)\n",
        "                data[edge_type].edge_index[0].append(u_index)\n",
        "                data[edge_type].edge_index[1].append(v_index)\n",
        "\n",
        "        print(\"Converting edge indices to tensors...\")\n",
        "        for edge_type in tqdm(data.edge_types, desc=\"Edge Types\"):\n",
        "            edge_index = data[edge_type].edge_index\n",
        "            data[edge_type].edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "G0F6rtsv1ItR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = torch.relu(x)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return torch.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "bEFKhytQ1SPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features\n",
        "le_card = LabelEncoder()\n",
        "le_merchant = LabelEncoder()\n",
        "df['card_id_enc'] = le_card.fit_transform(df['card_id'])\n",
        "df['merchant_enc'] = le_merchant.fit_transform(df['Merchant Name'])\n",
        "\n",
        "# Balancing the dataset without resetting the index\n",
        "fraudulent = df[df[\"Is Fraud?\"] == 1]\n",
        "non_fraudulent = df[df[\"Is Fraud?\"] == 0]\n",
        "\n",
        "desired_ratio = 1  # 2:1 balance\n",
        "non_fraud_sample_size = round(len(fraudulent) * desired_ratio)\n",
        "non_fraudulent_sampled = non_fraudulent.sample(n=non_fraud_sample_size, random_state=42)\n",
        "\n",
        "balanced_df = pd.concat([fraudulent, non_fraudulent_sampled])\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42)  # Do not reset index\n",
        "print(f\"Balanced dataset size: {len(balanced_df)}\")\n",
        "print(balanced_df[\"Is Fraud?\"].value_counts())\n",
        "\n",
        "df = balanced_df.copy()\n",
        "# Transform labels using the same encoders\n",
        "df['card_id_enc'] = le_card.transform(df['card_id'])\n",
        "df['merchant_enc'] = le_merchant.transform(df['Merchant Name'])\n",
        "\n",
        "# Split data into training and inductive sets\n",
        "cutoff = round(0.7 * len(df))\n",
        "train_data = df.iloc[:cutoff]\n",
        "inductive_data = df.iloc[cutoff:]\n",
        "\n",
        "# Prepare node features\n",
        "transaction_features = train_data.drop(columns=['card_id', 'Merchant Name', 'Is Fraud?', 'card_id_enc', 'merchant_enc'])\n",
        "transaction_features.index = train_data.index\n",
        "client_features = pd.DataFrame(index=train_data['card_id_enc'].unique())\n",
        "merchant_features = pd.DataFrame(index=train_data['merchant_enc'].unique())\n",
        "\n",
        "train_mask = torch.zeros(len(df), dtype=torch.bool)\n",
        "val_mask = torch.zeros(len(df), dtype=torch.bool)\n",
        "\n",
        "train_mask[:cutoff] = True  # Training nodes\n",
        "val_mask[cutoff:] = True\n",
        "\n",
        "# Create nodes and edges\n",
        "nodes = {\n",
        "    'client': df['card_id_enc'].unique(),\n",
        "    'merchant': df['merchant_enc'].unique(),\n",
        "    'transaction': df.index\n",
        "}\n",
        "edges = [\n",
        "    list(zip(df['card_id_enc'], df.index)),\n",
        "    list(zip(df.index, df['merchant_enc']))\n",
        "]\n",
        "features = {\n",
        "    'transaction': df.drop(columns=['card_id', 'Merchant Name', 'Is Fraud?', 'card_id_enc', 'merchant_enc']),\n",
        "    'client': pd.DataFrame(index=df['card_id_enc'].unique()),\n",
        "    'merchant': pd.DataFrame(index=df['merchant_enc'].unique())\n",
        "}\n",
        "\n",
        "# Build graph\n",
        "graph = GraphConstruction(nodes, edges, features)\n",
        "data = graph.to_pyg_data()\n",
        "\n",
        "data['transaction'].train_mask = train_mask\n",
        "data['transaction'].val_mask = val_mask\n",
        "data['transaction'].y = torch.tensor(df['Is Fraud?'].values, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5TjX5hi1TeJ",
        "outputId": "cb3e5100-8d9d-4127-bd9f-fc23c3b97da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced dataset size: 234\n",
            "Is Fraud?\n",
            "1    117\n",
            "0    117\n",
            "Name: count, dtype: int64\n",
            "Processing nodes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Nodes: 100%|██████████| 602/602 [00:00<00:00, 9138.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting node features to tensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Node Types: 100%|██████████| 3/3 [00:00<00:00, 348.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing edges...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Edges: 100%|██████████| 468/468 [00:00<00:00, 25593.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting edge indices to tensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Edge Types: 100%|██████████| 3/3 [00:00<00:00, 4550.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "in_channels = transaction_features.shape[1]\n",
        "hidden_channels = 64\n",
        "out_channels = 2  # Binary classification\n",
        "num_layers = 2\n",
        "model = GraphSAGE(in_channels, hidden_channels, out_channels, num_layers)\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Training loop\n",
        "model.train()\n",
        "target_labels = torch.tensor(train_data['Is Fraud?'].values, dtype=torch.long)  # Convert to tensor\n",
        "\n",
        "for epoch in range(201):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    out = model(data['transaction'].x, data[('transaction', 'to', 'merchant')].edge_index)\n",
        "    loss = criterion(out[data['transaction'].train_mask], data['transaction'].y[data['transaction'].train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRNw-Hrn491X",
        "outputId": "cb620438-15a7-41c8-84df-a1007fb3dba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.1475043296813965\n",
            "Epoch 11, Loss: 1.1132194995880127\n",
            "Epoch 21, Loss: 0.7265760898590088\n",
            "Epoch 31, Loss: 0.7345401048660278\n",
            "Epoch 41, Loss: 0.6320815086364746\n",
            "Epoch 51, Loss: 0.6047638058662415\n",
            "Epoch 61, Loss: 0.5975351333618164\n",
            "Epoch 71, Loss: 0.5924930572509766\n",
            "Epoch 81, Loss: 0.572880208492279\n",
            "Epoch 91, Loss: 0.6029146313667297\n",
            "Epoch 101, Loss: 0.6359283328056335\n",
            "Epoch 111, Loss: 0.8658685684204102\n",
            "Epoch 121, Loss: 0.6251945495605469\n",
            "Epoch 131, Loss: 0.58433598279953\n",
            "Epoch 141, Loss: 0.5790935754776001\n",
            "Epoch 151, Loss: 0.5592132210731506\n",
            "Epoch 161, Loss: 0.5712076425552368\n",
            "Epoch 171, Loss: 0.5983248353004456\n",
            "Epoch 181, Loss: 0.5569855570793152\n",
            "Epoch 191, Loss: 0.5679579377174377\n",
            "Epoch 201, Loss: 0.5701553821563721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_logits = out[data['transaction'].val_mask]\n",
        "    val_preds = torch.argmax(val_logits, dim=1)\n",
        "    val_labels = data['transaction'].y[data['transaction'].val_mask]\n",
        "print(val_labels.bincount())\n",
        "f1 = f1_score(val_labels.cpu(), val_preds.cpu(), average='binary')\n",
        "accuracy = accuracy_score(val_labels.cpu(), val_preds.cpu())\n",
        "conf_matrix = confusion_matrix(val_labels.cpu(), val_preds.cpu())\n",
        "\n",
        "print(f\"Validation F1 Score: {f1:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afR9vhZmaT2i",
        "outputId": "6768f266-1dd3-4834-e33f-446dbce6e442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([34, 36])\n",
            "Validation F1 Score: 0.7042\n",
            "Validation Accuracy: 0.7000\n",
            "Confusion Matrix:\n",
            "[[24 10]\n",
            " [11 25]]\n"
          ]
        }
      ]
    }
  ]
}